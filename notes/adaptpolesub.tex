\documentclass[11pt]{article}

\textwidth 6.5in
\oddsidemargin=0in
\evensidemargin=0in
\textheight 9in
\topmargin -0.5in

\usepackage{graphicx,bm,amssymb,amsmath,amsthm}
\usepackage{hyperref}

%\usepackage{showlabels}

\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\ben}{\begin{enumerate}}
\newcommand{\een}{\end{enumerate}}
\newcommand{\be}{\begin{equation}}
\newcommand{\ee}{\end{equation}}
\newcommand{\bea}{\begin{eqnarray}} 
\newcommand{\eea}{\end{eqnarray}}
\newcommand{\ba}{\begin{align}} 
\newcommand{\ea}{\end{align}}
\newcommand{\bse}{\begin{subequations}} 
\newcommand{\ese}{\end{subequations}}
\newcommand{\bc}{\begin{center}}
\newcommand{\ec}{\end{center}}
\newcommand{\bfi}{\begin{figure}}
\newcommand{\efi}{\end{figure}}
\newcommand{\ca}[2]{\caption{#1 \label{#2}}}
\newcommand{\ig}[2]{\includegraphics[#1]{#2}}
\newcommand{\tbox}[1]{{\mbox{\tiny #1}}}
\newcommand{\mbf}[1]{{\mathbf #1}}
\newcommand{\half}{\mbox{\small $\frac{1}{2}$}}
\newcommand{\vt}[2]{\left[\begin{array}{r}#1\\#2\end{array}\right]} % 2-col-vec
\newcommand{\mt}[4]{\left[\begin{array}{rr}#1&#2\\#3&#4\end{array}\right]} % 2x2
\newcommand{\eps}{\varepsilon}
\newcommand{\bigO}{{\mathcal O}}
\newcommand{\sfrac}[2]{\mbox{\small $\frac{#1}{#2}$}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\DeclareMathOperator{\re}{Re}
\DeclareMathOperator{\im}{Im}
\DeclareMathOperator{\tr}{Tr}
\DeclareMathOperator{\res}{res}
\newtheorem{thm}{Theorem}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{alg}[thm]{Algorithm}
\newtheorem{pro}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{rmk}[thm]{Remark}
\newtheorem{conj}[thm]{Conjecture}
% this work...
\newcommand{\om}{\omega}
\newcommand{\tH}{\tilde H}


\begin{document}
\title{Efficient adaptive quadrature for meromorphic integrands with nearby poles}
%  Brillouin zone integrands}
\author{Alex Barnett}
\date{\today}
\maketitle
\begin{abstract}
  % flip around to present numerical problem first?
  The computation of density of states and linear responses of crystalline
  quantum materials requires periodic integrals over a Brillouin zone such as $[0,2\pi)^3$.
  When using iterated integration,
  the innermost 1D integral has a meromorphic integrand
  with typically several poles of distance $\bigO(\eta)$ from the real axis,
  $\eta$ being a small broadening or temperature parameter.
  Conventional adaptive quadrature needs $\bigO(\log 1/\eta)$ subdivisions to
  handle each such pole, and a consequently large number of integrand evaluations.
  We present a pole-subtracting adaptive quadrature which, in contrast,
  needs around one subdivision per pole regardless of $\eta$.
  In practice this leads to an order of magnitude less integrand evaluations,
  and a commensurate acceleration factor in the case of more expensive integrands
  involving even small matrix inverses.
\end{abstract}

\section{Introduction}

We present a technique to defray the somewhat high
cost due to the many levels of geometric refinement induced by
an integrand with nearby poles in the complex plane
when using standard single variable adaptive integration.
Our motivating application is in quantum physics, although such meromorphic
integrands are expected to appear in other scientific applications such as
frequency-domain wave problems.

Consider a crystalline quantum system with band Hamiltonian $H(k)$.
Let $k$ denote the $2\pi$-periodic wavevector (a scalar since we are in 1D), and
the Brillouin zone we may take as any periodic interval, eg, $[0,2\pi)$.
We are handed a band Hamiltonian function in the form of a finite Fourier
series
\be
H(k) = \sum_{m=-M}^M H_m e^{imk}
\label{Hk}
\ee
which has $2M+1$ terms. $H$ and each $H_m$ are matrices of size $n$
(given in some Wannier basis that we need not be concerned about here).
We refer to the case $n=1$ as scalar.
For CCQ applications, $n$ is not large.
In the application, $H(k)$ is self-adjoint for all $k\in\R$,
equivalent to the symmetry $H_m^* = H_{-m}$ for all $m$, where
$^*$ indicates the complex transpose (Hermitian conjugate).

Given $M$, $\{H_m\}$ as above, $\om\in \R$, and $\eta\ge0$,
the paradigm integration task is to evaluate
\be
A = A(\om,\eta) \;:=\;
\int_{0}^{2\pi} \tr\, [(\om +i\eta)I - H(k)]^{-1} dk
\; = \;
\int_{0}^{2\pi} f(k) dk,
\label{A}
\ee
where we have abbreviated the integrand by $f$.
The inverse may be interpreted as a Green's function, and $\eta$
a possible broadening due to temperature, disorder, interactions, etc.
The density of states (DOS) at energy $\om$ is then
\be
\rho(\om) = -\frac{1}{\pi}\im A(\om,\eta).
\label{DOS}
\ee
% *** Tr and Im commute, but conceptuially shoudl the Tr be outermost?
The above is the simple case of constant matrix $(\om+i\eta)I$; this
can be a more general non-Hermitian matrix often denoted by
$\Sigma_\om$, still independent of $k$.
% although if not, that would just modify the F series coeffs for H.

The following is the key to our proposal for this application.
\begin{pro}
  Let $H$ be defined by \eqref{Hk}, with $n\ge 1$, and let $\om\in\R$, $\eta>0$.
  Then the scalar integrand $f$ in \eqref{A} is meromorphic throughout $\C$.
\end{pro}
\begin{proof}
  $(\omega+i\eta)I - H(k)$ is a holomorphic matrix-valued function of $k\in\C$,
  thus its inverse is a meromorphic matrix-valued function
  (see, eg, Smith normal form and the Keldysh theorem reviewed in
  \cite{beyn12,NEVPrev}).
  Its trace is therefore also meromorphic.
\end{proof}

A conventional numerical method for \eqref{A} is
to apply adaptive Gauss--Kronrod integration on the real axis.
This was proposed and tested for the innermost integral in \cite{autobz}.
When $\eta\ll1$ and $\om$ is in a band (meaning there is as least one
$k\in\R$ where $H(k)=\om$), this adaptive scheme is expected
to subdivide intervals down to a $k$-scale of about $\eta$, giving $\bigO(\log 1/\eta)$ cost.
We describe and test a faster method with uniform cost as $\eta\to 0$.

%\begin{rmk}
This note accompanies Julia benchmarking codes in the package implementation
\texttt{Int1DBZ} in\\
\url{https://github.com/ahbarnett/bz-integral}\\
This uses allocation-free Fourier series evaluators by Lorenzo van Mu\~noz.
  All experiments are on a Ryzen 2nd-gen 5700U laptop, 8-core, CPUMark score 2640 (1 thread), using AC power, with Julia 1.9.1, restricted to 1 thread.
%  \end{rmk}


% sssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssssss
\section{Methods}
\subsection{Pole subtraction on a single interval}

Consider the following integral over the standard interval,
and some $p$-node quadrature approximation,
\be
I := \int_{-1}^1 f(x) dx
\;\approx\;
I_p := \sum_{j=1}^p w_j f(x_j)
\label{I}
\ee
where $f$ is meromorphic, and $x_j$ and $w_j$ are nodes and weights for $[-1,1]$.
We take $p$ small, ie, less than 20.

We address only poles of $f$ lying in a Bernstein ellipse of parameter $\rho>0$,
recalling that its
semiaxes are $\cosh\rho$ and $\sinh\rho$.
Typically $\rho \approx 1.0$ works well.
Let all such poles of $f$ be simple (the generic case),
and let no pole of $f$ coincide with any $x_j$.
Let $f_j = f(x_j)$ be the given evaluations of the integrand.
The basic pole subtraction using only these evaluations proceeds as follows.
\ben
\item Find the coefficients $\mbf{c}$
  of a polynomial approximation $\tilde g(x)$ to $g(x) := 1/f(x)$.
  A simple way to do this that does not require any new function evaluations
  is via a backward-stable solve of the
  $p\times p$ Vandermonde system $V\mbf{c} = \mbf{g}$, where
  $V_{jk} = x_j^{k-1}$, for $j,k = 1,\dots,p$,
  the monomial coefficient vector is $\mbf{c} := \{c_0,\dots,c_{p-1}\}$,
  and the data vector is $\mbf{g} := \{1/f_1,\dots,1/f_p\}$.
  Then $\tilde g(x) = c_0 + c_1x + c_2 x^2 + \dots c_{p-1}x^{p-1}$.
\item
  Find all roots $r_k$, $k=1,\dots,K$
  of $\tilde g$ that lie within the Bernstein $\rho$-ellipse for $[-1,1]$.
  This can be a standard library call, but we also compare a custom method
  using Newton's method and deflation.
\item
  Extract the residues $R_k$, $k=1,\dots,K$ of each corresponding pole in $f$,
  simply the reciprocal of the derivative of the polynomial at the root:
  \be
  R_k = \frac{1}{\tilde g'(r_k)}~.
  \ee
  Since we assumed each pole was simple, each $R_k$ is finite.
\item Subtract each simple pole $R_k/(x-r_k)$ from $f(x)$, and instead
  add in its analytic integral over $[-1,1]$ namely $R_k \log[(1-r_k)/(-1-r_k)]$,
  using the usual branch cut for log.
  After subtraction, $f$ is analytic in the ellipse, and thus the quadrature
  rule is accurate.
  Summing over $K$ poles, the pole-subtracted rule is thus
  \be
  I^{\tbox{ps}}_p \;:=\;
  I_p +
  \sum_{k=1}^K R_k \biggl(
  \log\frac{1-r_k}{-1-r_k} -
  \sum_{j=1}^p w_j \frac{1}{x_j - r_k}
  \biggr).
  \label{ps}
  \ee
\een

\bfi % fffffffffffffffffffffffffffffffffffffffffffffffffff
\centering
\ig{width=4in}{spurious}
\ca{The case where a nearby root of $f$ causes $g=1/f$ to have a pole which
  induces spurious roots in the polynomial approximant $\tilde g$ of $g$.
  Here $f(x) = \cot \omega(x-z_0)$, with $z_0=0.3+10^{-3}i$, and a zero at
  $z_0-\pi/2$ is shown by the cross.}{f:spurious}
\efi
Some effort may be saved by breaking out of the above steps at step 2
if, say, $K>4$, since a large number of poles (approaching $p/2$)
cannot be reliably fit nor subtracted.

\subsection{Tests and failure modes of pole subtraction on one interval}

Here are some simple tests for non-rational functions showing successes and failures:
\bi
\item
  We check $f(x) = 1/\sin \omega(x-z_0)$ where a pole is at $z_0=0.3 + \delta i$,
  with $\delta = 10^{-3}$,
  and $\omega=2.0$ controls the density of poles.
  The line of poles is $z_0 + n\pi/\omega$ for $n\in\Z$, and all residues are
  $1/\omega$.
  Two poles are found in the $\rho=1.0$ Bernstein ellipse and subtracted.
  The integral $I$ from \eqref{I} is known analytically, and is of order unit sized.
  Using the $p=16$ Gauss--Legendre rule,
  pole-subtraction \eqref{ps} gives an error $|I^\tbox{ps}_p - I| \approx 5.5\times10^{-13}$.
  In contrast, adaptive GK using a $(7,15)$-node rule required 1185 function
  evaluations (and 40 final segments)
  to get an estimated error $10^{-12}$ (but actual error $10^{-15}$).
\item
  Changing to $\delta=10^{-10}$, the pole-subtraction error is hardly changed,
  but adaptive GK requires 5 million (!) function evaluations for 12 digits.
\item
  The nearest distance of a pole from any node was $0.018$ above.
  We now illustrate the ``pole-hitting'' failure mode.
  Moving to $z_0=x_{10} + i\delta$ minimizes this distance to become $\delta$.
  Then at
  $\delta=10^{-5}$ the error grows to $|I^\tbox{ps}_p - I| \approx 1.5\times 10^{-7}$,
  and appears to scale as $\bigO(\delta^{-2})$.
  This is presumably rounding error but the scaling needs to be understood.
\item
  The above function was the inverse of an entire function. However, an
  obvious failure mode of this scheme for general meromorphic $f$ is when $f$ has
  a root near $[-1,1]$,
  so that $g$ has a pole and hence $\tilde g$ is a bad approximation to $g$.
  We test this by switching to $f(x) = \cot \omega(x-z_0)$, which has the same
  poles as above, but also interleaved roots. Is it also analytically integrable.
  We change to $\omega=1$, so that
  the nearest root of $f$ is about $-1.27 + i\delta$.
  With $\rho=0.7$ there is one root $r_1$ of $\tilde g$ found, but its accuracy
  is only $2.4\times 10^{-6}$, and this is reflected by a poor error of
  $|I^\tbox{ps}_p - I| \approx 4\times 10^{-4}$.

  Setting $\rho=1$ encloses the nearest root of $f$ so that a spurious
  ellipse of roots of $g$ is found within the Bernstein ellipse; all $p$ roots
  (the maximum possible for $\tilde g$) are found, an indicator of failure.
  See Fig.~\ref{f:spurious}.
  In practice less than $p/2$
  poles may be subtracted without loss of accuracy.
\item
  A third type of failure is a higher-order pole in $f$.
  We square the original function to $f(x) = 1/\sin^2(\omega(x-z_0))$, also analytically
  integrable. We return to $\omega=2$, which gives two double-poles in the $\rho=1$
  ellipse. For $\delta \ge 10^{-5}$, pole subtraction gives error about $10^{-6}$.
  Each double pole is approximated by a pair of simple poles
  separated by around $10^{-6}$ and with residues of $10^4$
  (rather than their analytical infinite values).

  For $\delta<10^{-6}$ the method fails catastrophically (error $\bigO(\delta^{-1})$)
  because the separation of the
  pair of poles pushes one of them mistakenly
  across the real axis integration contour, causing
  a change of order the size of its residue.
  Without further knowledge of the order of the pole, one could not do much better,
  since the problem of polynomial
  root-finding has infinite condition number at a double root.
  By backward stability, given only double-precision data $1/f_j$
  it is probably unreasonable to get this topology correct.
  
  For such a double-pole the ``pole-hitting'' round-off error also exhibits worse scaling
  of $\bigO(\delta^{-4})$, and increases error to $3\times 10^{-4}$ at $\delta=10^{-3}$.
\ei
The above tests used \texttt{demo\_polesub.jl} with various uncommentings;
this code contains the analytic antiderivate formulae used for the error reporting.

In summary, there are three failure modes:
\ben
\item pole-hitting: an $\bigO(\eps_\tbox{mach}/d^2)$ roundoff error if any node
  is within distance $d$ of any pole.
\item near zero of $f$: such a pole in $1/f$ reduces the polynomial interpolation
  accuracy from the nodes on $[-1,1]$ in the usual fashion 
  according to its Bernstein $\rho$ parameter.
\item higher-order poles: this is relatively benign in the sense that the method
  still gets several digits of accuracy, unless the intrinsic ill-conditioning
\een

In all cases failure it is easy to diagnose.
*** discuss this using GK pair.

  
\bfi % fffffffffffffffffffffffffffffffffffffffffffffff  run: fig_polesegs.jl
\ig{width=\textwidth}{segsa}
\ig{width=\textwidth}{segsb}
\ca{Location of poles of integrand $f$ (red crosses),
  and comparison of final interval segmentation
  used by (a) plain adaptive GK vs (b) proposed pole-fitting adaptive GK.
  In (a) there are $n_f = 6045$ function evaluations (not shown) and 202 final segments
  (shown in black), whereas in (b), $n_f=675$ and there are 23 final segments
  (of which 11 are plain GK segments shown in black, and the other 12
  are pole-subtracting GK segments shown in green).
  Note the geometric subdivision in (a) near each of the
  eight poles near the real axis (corresponding to band intersections),
  and the $\bigO(1)$ pole-subtracting segment per pole in (b).
  The integrand $f(k)$ is defined from a matrix size
  $n=8$, maximum frequency $M=10$, temperature $\eta=10^{-5}$,
  tolerance $\eps=10^{-6}$, $\omega=0.5$, and iid normally-random
  Fourier coefficients with geometric decay to $10^{-2}$ by the maximum frequency.
  The function is the same in both cases, with the same GK $(7,15)$ rule.
  CPU times are 20ms for (a) and 3.3ms for (b).
}{f:polesegs}
\efi


\subsection{Adaptive integration with pole subtraction}
  
The goal is numerically to approximate with given relative tolerance $\eps$ the
integral
\be
A := \int_0^{2\pi} f(x) dx.
\label{Aagain}
\ee
We fix a Gauss rule with $q$ nodes nested into a Kronrod rule giving $2q+1$ total nodes.
We test with a standard choice $q=7$, meaning each segment uses $p=15$ evaluations,
appropriate for say 5--10 digit accuracies.
This gives an estimate $E_p$ of the error for $I_p$ by subtracting
the value using the nested Gauss rule from that using the Kronrod rule.
The same may be applied to \eqref{ps} to get an error estimate
$E^{\tbox{ps}}_p$ for $I^{\tbox{ps}}_p$.
For a segment $(a,b)$, a rescaling by $(b-a)$ is applied to the formulae
\eqref{I} and \eqref{ps}.
For any segment, the method with the minimum error is chosen, ie,
if $E^{\tbox{ps}}_p < E_p$, then
$I \leftarrow I^{\tbox{ps}}_p$ and $E \leftarrow E^{\tbox{ps}}_p$, otherwise
$I \leftarrow I_p$ and $E \leftarrow E_p$.
The values $I$ and $E$ are stored as properies of the segment.

The usual adaptive GK algorithm then proceeds:
as long as a global error estimate exceeds the user tolerance,
the segment with the largest $E$ is pulled from the heap,
split into two at its midpoint, the quadrature approximations for the two children
evaluated (via either standard GK or pole-subtracted GK, whichever has smaller $E$),
and the two children returned to the heap.
If pole-subtraction fails for any segment, either plain GK is successful
(eg in the neighborhood of a root of $f$ and relatively far from poles of $f$),
or refinement occurs until one of the two methods locally succeeds.

A simple demo using $H(k)$ with random decaying matrix-valued Fourier coefficients,
resulting in eight poles near the real axis,
is given in Fig.~\ref{f:polesegs},
with the parameters in the caption.
Note the replacement of refined panels by around 1 panel per nearby pole.
The function evaluations are reduced by 9x and the CPU time
of an efficient implementation is 6x faster.


\section{Results}

First some empirical timings in Julia for fixed degree $14$ ($p=15$ nodes).
The Vandermonde solve for $\mbf{c}$ with precomputed LU factorization
takes 1.5 $\mu$s.
We test various methods for rootfinding in Julia:
\ben
\item
  The library {\tt PolynomialRoots.jl}, using the Skowron--Gould algorithm \cite{skowron}.
  This takes around $10$--$20$ $\mu$s to find all roots of a degree $14$ polynomial.
  There is no way to speed up the library by limiting the number of poles found.
  We have found it reliable for the small degrees tested.
  We access this by {\tt meth="PR"}.  
\item A simple algorithm
{\tt few\_poly\_roots()}
  which removes roots sequentially by Newton rootfinding
  followed by deflation to factor out the root and reduce the degree by one,
  until a user-specified $n_r$ (default 3) roots are found.
  The first Newton guess starts at the node with minimum $1/f_j$; others start at
  $0$ with respect to the standard interval $[-1,1]$.
  The cost is $0.5$ $\mu$s times $n_r$.
  We access this by {\tt meth="F"}.
\een
We find that the latter is faster, but results in
poorer pole subtraction (more segment splitting).
Thus for expensive integrands ($n$ large, say $\ge 10$), the former is preferred.

\begin{table}   % ttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttt
  \centering
% now paste from term output of code...
\begin{tabular}{r|r|r|rrr|rrr|rr|}
&&& \multicolumn{3}{c}{standard GK} & \multicolumn{3}{c}{pole-sub. GK} & \multicolumn{2}{c}{ratios}\\ 
$M$ & $n$ & $t_\tbox{eval}$ ($\mu$s) & $n_\tbox{evals}$ & $t$ (ms) & $t_\tbox{over}$ ($\mu$s) & $n_\tbox{evals}$ & $t$ (ms) & $t_\tbox{over}$ ($\mu$s) & evals & time\\ 
\hline
10 & 1 & 0.035 & 3435 & 0.14 & 0.071 & 225 & 0.19 & 12 & 15 & 0.71\\ 
10 & 2 & 0.054 & 4335 & 0.24 & 0.024 & 375 & 0.32 & 12 & 12 & 0.75\\ 
10 & 4 & 0.11 & 5805 & 0.68 & 0.088 & 765 & 0.7 & 12 & 7.6 & 0.98\\ 
10 & 8 & 3.3 & 6045 & 20 & -0.42 & 795 & 3.3 & 12 & 7.6 & 6\\ 
\hline
100 & 1 & 0.24 & 29985 & 7.4 & 0.082 & 3105 & 3.2 & 12 & 9.7 & 2.3\\ 
100 & 2 & 0.26 & 20505 & 5.5 & 0.053 & 4995 & 5.3 & 12 & 4.1 & 1\\ 
100 & 4 & 0.61 & 42525 & 26 & 0.075 & 5565 & 7.8 & 12 & 7.6 & 3.3\\ 
100 & 8 & 7.3 & 52785 & 380 & -0.49 & 7485 & 62 & 14 & 7.1 & 6.2\\ 
\hline
\end{tabular}
% endpaste
\ca{Numbers of integrand evaluations $n_\tbox{evals}$, run-times $t$,
  and non-integrand-related overhead per segment $t_\tbox{over}$, comparing
  standard adaptive GK and our proposed pole-subtraction adaptive GK.
  Non-integer values are rounded to 2 significant digits.
  $t_\tbox{over}$ is estimated by dividing $t - n_\tbox{eval}t_\tbox{eval}$ by
  the number of segments $n_\tbox{eval}/15$.
  Negative overheads are artifacts of estimation and should be ignored.
  The third column shows time in microseconds per integrand evaluation $f(k)$.
The timings and tex for this table are generated by {\tt bench/timingtable.jl}.
}{t:time}
\end{table}

We fix $\eta=10^{-5}$ (a small but possible value in applications),
a target tolerance of $\eps=10^{-6}$,
and an exponential Fourier coefficient decay to $10^{-2}$ by $|m|=M$.
We explore integrand complexity ($M$) and matrix size ($n$).
The results are in Table~\ref{t:time}.
The improvement in integrand evaluations is around 10x (apart from $n=2$ at $M=100$,
strangely).
For low-complexity small $n$ there is no resulting speedup, presumably
due to the rootfinding and pole-subtraction cost per segment.
At $n=8$ the speedup is comparable to that expected by integrand evaluation ratio.
We see the gaps explained by a rather constant mean overhead of 12 $\mu$s per segment
for pole-subtraction.

Instead choosing $10^{-4}$ for the Fourier decay leads to less near-poles in $f$,
and slightly worse ratios and speedups. At $n=8$ the results are not affected much.

To do: test on $n=2$ graphene DOS example.



\section{Discussion}


Future ideas:
\bi
\item better benchmarking to break down the 12 $\mu$s
  CPU time overhead per segment for pole-subtraction
\item various code speedups: StaticArrays since node-number $p=15$ known and fixed.
\item only switch on pole-sub when segments sufficiently small; tried via {\tt maxpolesubint=1e-1} but made worse.
\item A explicit subtraction of higher-order poles may improve on the 3rd
  failure method found above.
\item A Boyd-type solution in the Joukowsky variable rather than the $x$ plane
  may be preferable, although the polynomial order will double. To try.
\item It may be possible that the (Fast)AAA rational approximation would be
  better than merely fitting roots of the reciprocal, on the same data $f_j$.
\item Middle integral branch-cut-avoiding adaptivity without pole subtraction.
\ei

\bibliographystyle{abbrv}
\bibliography{refs}

\end{document}
